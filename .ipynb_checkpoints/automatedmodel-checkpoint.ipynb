{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Python39\\lib\\site-packages\\mpl_finance.py:16: DeprecationWarning: \n",
      "\n",
      "  =================================================================\n",
      "\n",
      "   WARNING: `mpl_finance` is deprecated:\n",
      "\n",
      "    Please use `mplfinance` instead (no hyphen, no underscore).\n",
      "\n",
      "    To install: `pip install --upgrade mplfinance` \n",
      "\n",
      "   For more information, see: https://pypi.org/project/mplfinance/\n",
      "\n",
      "  =================================================================\n",
      "\n",
      "  __warnings.warn('\\n\\n  ================================================================='+\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import io, base64, os, json, re \n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import datetime\n",
    "from random import randint\n",
    "import math\n",
    "import pandas_datareader as web\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from pandas_datareader import data as pdr\n",
    "import fix_yahoo_finance\n",
    "from mpl_finance import candlestick_ohlc\n",
    "from matplotlib.dates import DateFormatter, date2num, WeekdayLocator, DateLocator, MONDAY\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from csv import writer\n",
    "import sys\n",
    "import yfinance as yahooFinance\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_call(stockname):\n",
    "    startdate = \"2010-01-01\"\n",
    "    enddate = \"2021-10-18\"\n",
    "    data = web.DataReader(stockname, data_source='yahoo',start=startdate,end=enddate)\n",
    "    data.reset_index(inplace=True,drop=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data):\n",
    "    new_data = []\n",
    "    for row_set in range(0, 100000):\n",
    "        if row_set%2000==0: print(row_set)\n",
    "        row_quant = randint(10, 30)\n",
    "        row_start = randint(0, len(data)-row_quant)\n",
    "        market_subset = data.iloc[row_start:row_start+row_quant]\n",
    "\n",
    "        Close_Date = max(market_subset['Date'])\n",
    "        if row_set%2000==0: print(Close_Date)\n",
    "        \n",
    "        Close_Gap = market_subset['Close'].pct_change()\n",
    "        High_Gap = market_subset['High'].pct_change()\n",
    "        Low_Gap = market_subset['Low'].pct_change() \n",
    "        Volume_Gap = market_subset['Volume'].pct_change() \n",
    "        Daily_Change = (market_subset['Close'] - market_subset['Open']) / market_subset['Open']\n",
    "        Outcome_Next_Day_Direction = (market_subset['Volume'].shift(-1) - market_subset['Volume'])\n",
    "        \n",
    "        new_data.append(pd.DataFrame({'ID':[row_set]*len(market_subset),\n",
    "                                    'Close_Date':[Close_Date]*len(market_subset),\n",
    "                                    'Close_Gap':Close_Gap,\n",
    "                                    'High_Gap':High_Gap,\n",
    "                                    'Low_Gap':Low_Gap,\n",
    "                                    'Volume_Gap':Volume_Gap,\n",
    "                                    'Daily_Change':Daily_Change,\n",
    "                                    'Outcome_Next_Day_Direction':Outcome_Next_Day_Direction}))\n",
    "    return market_subset,new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_grid(compressed_grid, unique_patterns):\n",
    "\n",
    "    patterns = []\n",
    "    counts = []\n",
    "    for from_event in unique_patterns:\n",
    "\n",
    "        for to_event in unique_patterns:\n",
    "            pattern = from_event + ',' + to_event \n",
    "\n",
    "            ids_matches = compressed_grid[compressed_grid['Event_Pattern'].str.contains(pattern)]\n",
    "            found = 0\n",
    "            if len(ids_matches) > 0:\n",
    "                Event_Pattern = '---'.join(ids_matches['Event_Pattern'].values)\n",
    "                found = Event_Pattern.count(pattern)\n",
    "            patterns.append(pattern)\n",
    "            counts.append(found)\n",
    "\n",
    "    grid_data = pd.DataFrame({'pairs':patterns, 'counts': counts})\n",
    "\n",
    "    para = grid_data['pairs'].str.split(',', 1)\n",
    "    grid_data['x'], grid_data['y'] = para.str[0],para.str[1]\n",
    "    grid_data.head()\n",
    "\n",
    "    grid_data = grid_data.pivot(index='x', columns='y', values='counts')\n",
    "\n",
    "    grid_data.columns= [col for col in grid_data.columns]\n",
    "    grid_data.fillna(0, inplace=True)\n",
    "    grid_data.head()\n",
    "    grid_data = grid_data / grid_data.sum(1)\n",
    "    return (grid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess(market_subset,new_data):\n",
    "    new_set_data = pd.concat(new_data)\n",
    "    print(new_set_data.shape)\n",
    "    new_set_data = new_set_data.dropna(how='any') \n",
    "    print(new_set_data.shape)\n",
    "    new_set_data['Close_Gap_LMH'] = pd.qcut(new_set_data['Close_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "    new_set_data['High_Gap_LMH'] = pd.qcut(new_set_data['High_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "    new_set_data['Low_Gap_LMH'] = pd.qcut(new_set_data['Low_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "    new_set_data['Volume_Gap_LMH'] = pd.qcut(new_set_data['Volume_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "    new_set_data['Daily_Change_LMH'] = pd.qcut(new_set_data['Daily_Change'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "    new_set_data = new_set_data[[\"ID\",\"Close_Date\",\"Close_Gap_LMH\",\"Volume_Gap_LMH\",\"Daily_Change_LMH\",\"Outcome_Next_Day_Direction\"]]\n",
    "    new_set_data['Event_Pattern'] = new_set_data['Close_Gap_LMH'].astype(str) + new_set_data['Volume_Gap_LMH'].astype(str) + new_set_data['Daily_Change_LMH'].astype(str)\n",
    "\n",
    "    compressed_set = new_set_data.groupby(['ID','Close_Date'])['Event_Pattern'].apply(lambda x: \"{%s}\" % ', '.join(x)).reset_index()\n",
    "    print(compressed_set.shape)\n",
    "\n",
    "    compressed_outcomes = new_set_data.groupby(['ID', 'Close_Date'])['Outcome_Next_Day_Direction'].mean()\n",
    "    compressed_outcomes = compressed_outcomes.to_frame().reset_index()\n",
    "    print(compressed_outcomes.shape)\n",
    "\n",
    "    compressed_set = pd.merge(compressed_set, compressed_outcomes, on= ['ID', 'Close_Date'], how='inner')\n",
    "    print(compressed_set.shape)\n",
    "\n",
    "    compressed_set['Event_Pattern'] = [''.join(e.split()).replace('{','').replace('}','') for e in compressed_set['Event_Pattern'].values]\n",
    "\n",
    "    compressed_set_validation = compressed_set[compressed_set['Close_Date'] >= datetime.datetime.now() - datetime.timedelta(days=270)]\n",
    "    print(compressed_set_validation.shape)\n",
    "\n",
    "    compressed_set = compressed_set[['ID', 'Event_Pattern','Outcome_Next_Day_Direction']]\n",
    "    compressed_set_validation = compressed_set_validation[['ID', 'Event_Pattern','Outcome_Next_Day_Direction']]\n",
    "\n",
    "    print(len(compressed_set['Outcome_Next_Day_Direction']))\n",
    "    len(compressed_set[abs(compressed_set['Outcome_Next_Day_Direction']) > 10000000])\n",
    "\n",
    "    print('all moves:', len(compressed_set))\n",
    "    compressed_set = compressed_set[abs(compressed_set['Outcome_Next_Day_Direction']) > 10000000]\n",
    "    compressed_set['Outcome_Next_Day_Direction'] = np.where((compressed_set['Outcome_Next_Day_Direction'] > 0), 1, 0)\n",
    "    compressed_set_validation['Outcome_Next_Day_Direction'] = np.where((compressed_set_validation['Outcome_Next_Day_Direction'] > 0), 1, 0)\n",
    "    print('big moves only:', len(compressed_set))  \n",
    "\n",
    "    compressed_set_pos = compressed_set[compressed_set['Outcome_Next_Day_Direction']==1][['ID', 'Event_Pattern']]\n",
    "    print(compressed_set_pos.shape)\n",
    "    compressed_set_neg = compressed_set[compressed_set['Outcome_Next_Day_Direction']==0][['ID', 'Event_Pattern']]\n",
    "    print(compressed_set_neg.shape)\n",
    "\n",
    "    flat_list = [item.split(',') for item in compressed_set['Event_Pattern'].values ]\n",
    "    unique_patterns = ','.join(str(r) for v in flat_list for r in v)\n",
    "    unique_patterns = list(set(unique_patterns.split(',')))\n",
    "    len(unique_patterns)\n",
    "    grid_pos = build_transition_grid(compressed_set_pos, unique_patterns) \n",
    "    grid_neg = build_transition_grid(compressed_set_neg, unique_patterns)\n",
    "    return(compressed_set_validation,grid_pos,grid_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x,y):\n",
    "   try:\n",
    "      lg = np.log(x/y)\n",
    "   except:\n",
    "      lg = 0\n",
    "   return lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def putdataexcel(stockname,startdate,enddate,Accuracy):\n",
    "    result_list = [stockname,startdate,enddate,Accuracy]\n",
    "    with open('result.csv', 'a') as result_data:\n",
    "        write_data = writer(result_data)\n",
    "        write_data.writerow(result_list)\n",
    "        result_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2011-02-11 00:00:00\n",
      "2000\n",
      "2010-09-06 00:00:00\n",
      "4000\n",
      "2018-09-07 00:00:00\n",
      "6000\n",
      "2015-02-02 00:00:00\n",
      "8000\n",
      "2017-08-14 00:00:00\n",
      "10000\n",
      "2020-02-28 00:00:00\n",
      "12000\n",
      "2017-02-16 00:00:00\n",
      "14000\n",
      "2010-10-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "filename = 'stock_list.csv'\n",
    "with open(filename, newline='', encoding='utf-8-sig') as file1:\n",
    "    reader = csv.reader(file1)\n",
    "    for row in reader:\n",
    "        stock_name  = ''.join(map(str,row))\n",
    "        data  = data_call(stock_name)\n",
    "        market_subset,new_data = data_process(data)\n",
    "        compressed_set_validation,grid_pos,grid_neg = dataprocess(market_subset,new_data)\n",
    "        actual = []\n",
    "        predicted = []\n",
    "        for seq_id in compressed_set_validation['ID'].values:\n",
    "            patterns = compressed_set_validation[compressed_set_validation['ID'] == seq_id]['Event_Pattern'].values[0].split(',')\n",
    "            pos = []\n",
    "            neg = []\n",
    "            log_odds = []\n",
    "            for id in range(0, len(patterns)-1):\n",
    "                numerator = grid_pos[patterns[id]][patterns[id+1]]\n",
    "                denominator = grid_neg[patterns[id]][patterns[id+1]]\n",
    "                if (patterns[id] in list(grid_pos) and patterns[id+1] in list(grid_pos) and patterns[id] in list(grid_neg) and patterns[id+1] in list(grid_neg)):      \n",
    "                    if (numerator == 0 and denominator == 0):\n",
    "                        log_value =0\n",
    "                    elif (denominator == 0):\n",
    "                        log_value = np.log(numerator / 0.00001)\n",
    "                    elif (numerator == 0):\n",
    "                        log_value = np.log(0.00001 / denominator)\n",
    "                    else:\n",
    "                        log_value = np.log(numerator/denominator)\n",
    "                else:\n",
    "                    log_value = 0\n",
    "                log_odds.append(log_value)\n",
    "                pos.append(numerator)\n",
    "                neg.append(denominator)  \n",
    "            print('outcome:', compressed_set_validation[compressed_set_validation['ID']==seq_id]['Outcome_Next_Day_Direction'].values[0])\n",
    "            print(sum(pos)/sum(neg))\n",
    "            print(sum(log_odds))\n",
    "            actual.append(compressed_set_validation[compressed_set_validation['ID']==seq_id]['Outcome_Next_Day_Direction'].values[0])\n",
    "            predicted.append(sum(log_odds))\n",
    "        confusion_matrix(actual, [1 if p > 0 else 0 for p in predicted])\n",
    "        score = accuracy_score(actual, [1 if p > 0 else 0 for p in predicted])\n",
    "        Accuracy = round(score * 100,2)\n",
    "        print('Accuracy:',Accuracy , '%')\n",
    "        putdataexcel(stock_name,\"2010-01-01\",\"2021-10-18\",Accuracy)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = pd.read_csv('result.csv')\n",
    "total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Model Accuracy(%) :', total_acc['Accuracy(%)'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d008810b9c8467bcb3ca39aa2180e5b81b3a9acb136aab30d47954377cc5120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
